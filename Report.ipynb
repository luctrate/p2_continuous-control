{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student: Steven Hooker\n",
    "\n",
    "Project: Continuous Control - Reacher\n",
    "\n",
    "Course: Deep Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this project is to train an agent to solve the \"Reacher\" environment. Within this environment the agent needs to control a double joint arm and reach target locations.\n",
    "\n",
    "Following the given description of the challenge. \n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Solve the Second Version\n",
    "The barrier for solving the second version of the environment is slightly different, to take into account the presence of many agents. In particular, your agents must get an average score of +30 (over 100 consecutive episodes, and over all agents). Specifically,\n",
    "\n",
    "After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 20 (potentially different) scores. We then take the average of these 20 scores.\n",
    "This yields an average score for each episode (where the average is over all 20 agents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent & Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDPG Agent\n",
    "\n",
    "In order to solve the challenge  the DDPG was chosen to be impplemented. \n",
    "[DDPG paper](https://arxiv.org/pdf/1509.02971). \n",
    "\n",
    "\n",
    "* DDPG alongside with xy belongs to the actor-critc methods. \n",
    "* Actor-Critic methods try to combine value based and policy based methods as each have their different advantages and hence a try worth merging. \n",
    "* While policy-based methods are better for continuous and stochastic environments and learning on how to act (Actor) value based methods try to estimate if situations are good or bad (Critc) and are more steady.\n",
    "* Combining policy and value based methods yields in general to lower variance and bias during the learning process and to better agents.\n",
    "\n",
    "\n",
    "![dppg.png](./assets/ddpg.png)\n",
    "\n",
    "Source: [DDPG paper](https://arxiv.org/pdf/1509.02971) Algorithm 1\n",
    "\n",
    "\n",
    "#### Model \n",
    "\n",
    "    Actor network parameters\n",
    "    ```\n",
    "    ----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "    ================================================================\n",
    "           BatchNorm1d-1                   [-1, 33]              66\n",
    "                Linear-2                  [-1, 256]           8,704\n",
    "                Linear-3                  [-1, 128]          32,896\n",
    "                Linear-4                    [-1, 4]             516\n",
    "    ================================================================\n",
    "    Total params: 42,182\n",
    "    Trainable params: 42,182\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.00\n",
    "    Forward/backward pass size (MB): 0.00\n",
    "    Params size (MB): 0.16\n",
    "    Estimated Total Size (MB): 0.16\n",
    "    ----------------------------------------------------------------\n",
    "    ```\n",
    "    \n",
    "    Critic network parameters\n",
    "    ```\n",
    "    ----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "    ================================================================\n",
    "           BatchNorm1d-1                   [-1, 33]              66\n",
    "                Linear-2                  [-1, 256]           8,704\n",
    "                Linear-3                  [-1, 128]          33,408\n",
    "                Linear-4                    [-1, 1]             129\n",
    "    ================================================================\n",
    "    Total params: 42,307\n",
    "    Trainable params: 42,307\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.00\n",
    "    Forward/backward pass size (MB): 0.00\n",
    "    Params size (MB): 0.16\n",
    "    Estimated Total Size (MB): 0.17\n",
    "    ----------------------------------------------------------------\n",
    "    ```\n",
    "\n",
    "#### Soft Update  & Temporal Difference \n",
    "\n",
    "* Soft update is employed for both actor and critic network to achieve a stable learning process. The parameter of $\\tau$ is used to control how much we mix in from the behavior network into the target network each time.\n",
    "\n",
    "    $$\n",
    "    \\theta_{target} = \\tau*\\theta_{local} + (1-\\tau)*\\theta_{target}\n",
    "    $$\n",
    "    \n",
    "* The critic network is learned with temporal difference(TD) approach.\n",
    "    $$\n",
    "    y_t = r_t + discount * Q'(s_{t+1},a,\\theta_t')\n",
    "    $$    $$\n",
    "    L^{critic} = \\frac{1}{N}\\sum(y_t - Q(s_t,a,\\theta_t))^2\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "Following hyperparameters where used to solve this challenge.\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "```\n",
    "\n",
    "During the project different hyperparameters where tested. \n",
    "For example:\n",
    "Increasing TAU lead to a more volatile learning experience, sometimes from 10 episodes to next 10 episodes the average score increased or dropped by 50% even in the later stages. With the current TAU the average score increased slower but more consistently. \n",
    "\n",
    "The code and instructions on how start, use the trained agent and train the agent from scratch can be founds here. \n",
    "https://github.com/luctrate/p2_continuous-control\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The following plot shows the score over the episodes and that the challenge was solved after 16 episodes. Why 16? Average score from episode 17 to 116 is over 30. \n",
    "\n",
    "ps.: There is a minor error in the challenge discription in the example when the challenge is considered being solved. They are not considering the average and mainly looking at the first time the score passes 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![results.png](./assets/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancements\n",
    "Unfortunately, getting the DDPG Agent to solve the challenge took me longer than expected. I am interested in using PPO to tackle the challenge and see how it compares to to the current solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
